{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-04T08:25:38.203264Z",
     "start_time": "2025-05-04T08:25:34.723902Z"
    }
   },
   "source": [
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.models import DeepGraphInfomax\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import graph_reinforcement_learning_using_blockchain_data as grl\n",
    "from graph_reinforcement_learning_using_blockchain_data import config\n",
    "\n",
    "config.load_dotenv()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-04 10:25:37.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgraph_reinforcement_learning_using_blockchain_data.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mPROJ_ROOT path is: /Users/liamtessendorf/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "5f0d4d1f94b325a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:25:48.996567Z",
     "start_time": "2025-05-04T08:25:38.279059Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = torch.load(config.FLASHBOTS_Q2_DATA_DIR / \"state_graphs.pt\", weights_only=False)",
   "id": "941251425b63bfac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:25:49.094399Z",
     "start_time": "2025-05-04T08:25:49.010053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_feats_len = 0\n",
    "for vals in dataset.values():\n",
    "    for graphs in vals:\n",
    "        if graphs.x.shape[1] > max_feats_len:\n",
    "            max_feats_len = graphs.x.shape[1]"
   ],
   "id": "abb3e8e52d9f6b21",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:25:50.111590Z",
     "start_time": "2025-05-04T08:25:49.101489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for vals in dataset.values():\n",
    "    for graphs in vals:\n",
    "        graphs.x = grl.pad_features(graphs.x, max_feats_len)"
   ],
   "id": "3fce62f876bb81d8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:25:50.161728Z",
     "start_time": "2025-05-04T08:25:50.128151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_list = [d for sublist in dataset.values() for d in sublist]\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    data_list, [0.8, 0.2], generator=generator\n",
    ")"
   ],
   "id": "894446a3b1e749e2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:25:50.174660Z",
     "start_time": "2025-05-04T08:25:50.171194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=True, exclude_keys=[\"account_mapping\"], drop_last=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, exclude_keys=[\"account_mapping\"], drop_last=False\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_list, batch_size=256, shuffle=False, exclude_keys=[\"account_mapping\"], drop_last=False\n",
    ")"
   ],
   "id": "b66e5359dca52f40",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train DGI",
   "id": "62d58667144704c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:27:47.292348Z",
     "start_time": "2025-05-04T08:27:47.288995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = lambda z, *_, batch=None, **__: global_mean_pool(z, batch)\n",
    "\n",
    "\n",
    "def corruption(data):\n",
    "    \"\"\"\n",
    "    Corrupts the graph data by shuffling node features.\n",
    "    Accepts a Data/Batch object and returns a corrupted Data/Batch object.\n",
    "    \"\"\"\n",
    "    corrupted_data = data.clone()  # Create a copy to avoid modifying the original data\n",
    "    num_nodes_total = data.x.size(0)\n",
    "    # Generate a permutation of node indices within the batch\n",
    "    perm = torch.randperm(num_nodes_total, device=data.x.device)\n",
    "    corrupted_data.x = data.x[perm]  # Apply the permutation to shuffle features\n",
    "    return corrupted_data"
   ],
   "id": "832aef3fe4ed654d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:27:47.557569Z",
     "start_time": "2025-05-04T08:27:47.528391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_node_features = max_feats_len\n",
    "dim_global_features = 0\n",
    "hidden_channels = 128\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "enc = grl.SAGEEncoder(num_node_features, hidden_channels).to(device)\n",
    "dgi = DeepGraphInfomax(\n",
    "    hidden_channels=hidden_channels, encoder=enc, summary=summary, corruption=corruption\n",
    ").to(device)"
   ],
   "id": "104a3f0439022550",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T10:02:06.080223Z",
     "start_time": "2025-05-03T09:26:57.836133Z"
    }
   },
   "cell_type": "code",
   "source": "enc = grl.pretrain_dgi(train_loader, dgi, device, epochs=10)",
   "id": "fb8fb3cf0bf183ce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/03 11:26:57 WARNING mlflow.utils.autologging_utils: MLflow pytorch autologging is known to be compatible with 1.9.0 <= torch <= 2.5.1, but the installed version is 2.6.0. If you encounter errors during autologging, try upgrading / downgrading torch to a compatible version, or try upgrading MLflow.\n",
      " 10%|â–ˆ         | 1/10 [02:00<18:06, 120.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 01  loss=0.3962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 2/10 [04:32<18:33, 139.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 02  loss=0.1031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [07:46<19:08, 164.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 03  loss=0.0875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [10:39<16:46, 167.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 04  loss=0.0674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [13:27<13:58, 167.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 05  loss=0.0567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [16:17<11:13, 168.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 06  loss=0.0508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [22:37<11:53, 237.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 07  loss=0.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [27:37<08:35, 257.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 08  loss=0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [31:09<04:03, 243.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 09  loss=0.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [35:01<00:00, 210.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGI] epoch 10  loss=0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31m2025/05/03 12:02:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run DGI 128 at: http://127.0.0.1:8080/#/experiments/330930495026013213/runs/dc95b44ca9ad4acdbe557b133f6b6653\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8080/#/experiments/330930495026013213\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train GraphSAGE Classifier using frozen DGI embeddings",
   "id": "faeecdc527a046f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:27:38.398958Z",
     "start_time": "2025-05-04T08:27:38.289448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_uri = \"mlflow-artifacts:/330930495026013213/1f3f3d97f6a14569b9f2f285c5a98bb4/artifacts/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "enc = model.encoder"
   ],
   "id": "a0c8e058008db486",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10838495d9424c02b83545debd5a171c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:27:50.619919Z",
     "start_time": "2025-05-04T08:27:50.611305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in enc.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model = grl.GraphSAGEClassifier(enc, hidden=128, num_classes=2).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)"
   ],
   "id": "b3995451efaecf8e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:31:53.447762Z",
     "start_time": "2025-05-04T08:31:53.134662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, embeddings = grl.run_experiment(\n",
    "    \"Unsupervised GraphSAGE with DGI\",\n",
    "    10,\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    return_embeddings=True,\n",
    ")"
   ],
   "id": "c901a788e096367a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 10:31:53 WARNING mlflow.utils.autologging_utils: MLflow pytorch autologging is known to be compatible with 1.9.0 <= torch <= 2.5.1, but the installed version is 2.6.0. If you encounter errors during autologging, try upgrading / downgrading torch to a compatible version, or try upgrading MLflow.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starts\n",
      "ðŸƒ View run powerful-whale-206 at: http://127.0.0.1:8080/#/experiments/132032870842317128/runs/82a148189d3648e0b92db81d4eb3821e\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8080/#/experiments/132032870842317128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (256x128 and 256x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mgrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnsupervised GraphSAGE with DGI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data/graph_reinforcement_learning_using_blockchain_data/modeling/gnn.py:213\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(experiment_name, num_epochs, model, train_loader, test_loader, optimizer, criterion, device, return_embeddings)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m starts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     test_loss, test_acc, embeddings \u001b[38;5;241m=\u001b[39m test(\n\u001b[1;32m    215\u001b[0m         model, test_loader, criterion, device, return_embeddings\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    218\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss, step\u001b[38;5;241m=\u001b[39mepoch)\n",
      "File \u001b[0;32m~/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data/graph_reinforcement_learning_using_blockchain_data/modeling/gnn.py:129\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# data.x = torch.cat([data.x, data.global_features[data.batch].unsqueeze(1)], dim=-1)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 129\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m    131\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data/graph_reinforcement_learning_using_blockchain_data/modeling/gnn.py:82\u001b[0m, in \u001b[0;36mGraphSAGEClassifier.forward\u001b[0;34m(self, data, return_embeddings)\u001b[0m\n\u001b[1;32m     80\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(data)\n\u001b[1;32m     81\u001b[0m x \u001b[38;5;241m=\u001b[39m global_mean_pool(x, data\u001b[38;5;241m.\u001b[39mbatch, size\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mnum_graphs)\n\u001b[0;32m---> 82\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(embeddings, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m     84\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Programming/Uni/2_Master/4_FS25_Programming/graph-reinforcement-learning-using-blockchain-data/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (256x128 and 256x128)"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Embeddings from pre-trained GNN for downstream tasks",
   "id": "d2aa71367420f46f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:32:31.595782Z",
     "start_time": "2025-05-04T08:32:31.475126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_uri = \"mlflow-artifacts:/132032870842317128/ba4bedca3761489498f42f57c3986752/artifacts/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)"
   ],
   "id": "e3758a90170ab65d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aad33ab8d4ad4d56a2dd7532b862d4ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:33:33.322301Z",
     "start_time": "2025-05-04T08:32:32.413011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "_, _, embeddings = grl.test(model, data_loader, criterion, device, return_embeddings=True)"
   ],
   "id": "616597b175f999fa",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:33:57.781069Z",
     "start_time": "2025-05-04T08:33:40.237327Z"
    }
   },
   "cell_type": "code",
   "source": "emb = {trx_id: emb.cpu().detach().numpy().tolist() for trx_id, emb in embeddings.items()}",
   "id": "ea2d7aa9672bc5ad",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:33:57.963508Z",
     "start_time": "2025-05-04T08:33:57.940817Z"
    }
   },
   "cell_type": "code",
   "source": "df_embeddings = pd.DataFrame({\"transactionHash\": emb.keys(), \"embeddings\": emb.values()})",
   "id": "6023588c0ec43e0c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:34:20.619636Z",
     "start_time": "2025-05-04T08:34:10.286689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_embeddings.to_csv(\n",
    "    config.FLASHBOTS_Q2_DATA_DIR / \"state_embeddings_pre_trained_128.csv\", index=False\n",
    ")"
   ],
   "id": "b828eb03af4e689e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Embeddings from DGI encoder for downstream tasks",
   "id": "5a51dddd0feacfa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T12:17:24.592978Z",
     "start_time": "2025-05-03T12:17:24.245829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_uri = \"mlflow-artifacts:/330930495026013213/dc95b44ca9ad4acdbe557b133f6b6653/artifacts/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)"
   ],
   "id": "931d7e1a0dc8c4d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df44d739f13e4acf948b306101f62a76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T12:27:54.972313Z",
     "start_time": "2025-05-03T12:17:27.253020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "unsup_list, y_list = [], []\n",
    "device = torch.device(\"mps\")\n",
    "embeddings = {}\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(data_loader):\n",
    "        data = data.to(device)\n",
    "\n",
    "        z_nodes = model.encoder(data)\n",
    "        unsup_emb = global_mean_pool(z_nodes, data.batch)\n",
    "\n",
    "        mapping = {trx_id: emb for trx_id, emb in zip(data.trx_id, unsup_emb)}\n",
    "        embeddings.update(mapping)\n",
    "\n",
    "        unsup_list.append(unsup_emb.cpu())\n",
    "        y_list.append(data.y.cpu())\n",
    "\n",
    "unsup_X = torch.cat(unsup_list).numpy()\n",
    "y = torch.cat(y_list).numpy()"
   ],
   "id": "f230a6819574b0a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 583/583 [10:27<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T12:36:58.745163Z",
     "start_time": "2025-05-03T12:36:27.818822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emb = {trx_id: emb.cpu().detach().numpy().tolist() for trx_id, emb in embeddings.items()}\n",
    "df_embeddings = pd.DataFrame({\"transactionHash\": emb.keys(), \"embeddings\": emb.values()})\n",
    "\n",
    "df_embeddings.to_csv(config.FLASHBOTS_Q2_DATA_DIR / \"state_embeddings_dgi_128.csv\", index=False)"
   ],
   "id": "71e813129275ff44",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
